{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90487ded",
   "metadata": {},
   "source": [
    "# 04 — LLM-Based Review Summarization\n",
    "\n",
    "## Objective\n",
    "Generate category-level blog-style summaries from Amazon product reviews using an instruction-tuned LLM (LLaMA 3.2-3B).\n",
    "\n",
    "This notebook:\n",
    "- Aggregates reviews at product level\n",
    "- Generates structured blog articles per meta-category\n",
    "- Compares decoding strategies (controlled vs creative)\n",
    "- Analyzes hallucination and data noise effects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e1da9f",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "- GPU: Colab L4\n",
    "- Model: LLaMA 3.2-3B-Instruct\n",
    "- Inference only (no fine-tuning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8516646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup (First Run Only)\n",
    "\n",
    "# !pip install transformers\n",
    "# !pip install torch\n",
    "# !pip install rouge-score\n",
    "# !pip install bert-score\n",
    "# !pip install sentencepiece\n",
    "# !pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e0fc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bertscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7a3719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device: NVIDIA L4\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f499fa7",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "We:\n",
    "1. Load processed electronics dataset\n",
    "2. Re-apply meta-category assignment (for reproducibility)\n",
    "3. Aggregate reviews at product level to reduce cross-product noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3591a77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo updated.\n",
      "File exists: True\n"
     ]
    }
   ],
   "source": [
    "REPO = \"https://github.com/marcosfsousa/project-ironhack-automated-customer-reviews.git\"\n",
    "\n",
    "if not os.path.exists(\"/content/repo\"):\n",
    "    subprocess.run([\"git\", \"clone\", REPO, \"/content/repo\"], check=True)\n",
    "    print(\"Repo cloned.\")\n",
    "else:\n",
    "    subprocess.run([\"git\", \"-C\", \"/content/repo\", \"pull\"], check=True)\n",
    "    print(\"Repo updated.\")\n",
    "\n",
    "DATA_PATH = \"/content/repo/data/processed/electronics_ready.csv\"\n",
    "print(f\"File exists: {os.path.exists(DATA_PATH)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b08fdfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30487, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>brand</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>5</td>\n",
       "      <td>This product so far has not disappointed. My c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>5</td>\n",
       "      <td>great for beginner or experienced person. Boug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>5</td>\n",
       "      <td>Inexpensive tablet for him to use and learn on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>4</td>\n",
       "      <td>I've had my Fire HD 8 two weeks now and I love...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>5</td>\n",
       "      <td>I bought this for my grand daughter when she c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name   brand  rating  \\\n",
       "0  All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...  Amazon       5   \n",
       "1  All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...  Amazon       5   \n",
       "2  All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...  Amazon       5   \n",
       "3  All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...  Amazon       4   \n",
       "4  All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi,...  Amazon       5   \n",
       "\n",
       "                                         review_text  \n",
       "0  This product so far has not disappointed. My c...  \n",
       "1  great for beginner or experienced person. Boug...  \n",
       "2  Inexpensive tablet for him to use and learn on...  \n",
       "3  I've had my Fire HD 8 two weeks now and I love...  \n",
       "4  I bought this for my grand daughter when she c...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/processed/electronics_ready.csv\")\n",
    "print(df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda57185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ccc809974c24e3eb017b3200b4d2a63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ecb53a26a9d47bf84908cd4f2acb178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e70c25f665b497b82cdcfab3bc1d73a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b09a229c54d4933bc5c75d6019c79ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7417c826ffdc47238ead2ee8a45fc576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d79c10ae22e0453390ab5199505a0c01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "117eafd15c5e431b8a1e0109766b9ec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da93b57dbbe249ed9e8b2192ff893a73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1077376b0e004fabbb5f3c99101fbc33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "#MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" # Public access model used while gated model authorization was being reviewed\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\" # Gated model that requires auth from the Repo Owners in HF\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(\"LLaMA model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d25be15",
   "metadata": {},
   "source": [
    "### Why Aggregate at Product Level?\n",
    "\n",
    "Initial generation attempts revealed review contamination across categories.\n",
    "To reduce noise, we:\n",
    "- First aggregate reviews per product\n",
    "- Then aggregate top-N products per category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df172592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-category distribution:\n",
      "meta_category\n",
      "Fire Tablets             19739\n",
      "Echo & Smart Speakers     4262\n",
      "Kindle E-Readers          4231\n",
      "Fire Kids Edition         2191\n",
      "Accessories & Other         58\n",
      "Fire TV & Streaming          6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Products in 'Accessories & Other': 9\n",
      "<StringArray>\n",
      "[                         'Coconut Water Red Tea 16.5 Oz (pack of 12)',\n",
      "                     'AmazonBasics Nylon CD/DVD Binder (400 Capacity)',\n",
      "                     'AmazonBasics Ventilated Adjustable Laptop Stand',\n",
      "                   'AmazonBasics Backpack for Laptops up to 17-inches',\n",
      "                                'AmazonBasics 11.6-Inch Laptop Sleeve',\n",
      "                               'AmazonBasics External Hard Drive Case',\n",
      " 'AmazonBasics USB 3.0 Cable - A-Male to B-Male - 6 Feet (1.8 Meters)',\n",
      "                       'AmazonBasics 16-Gauge Speaker Wire - 100 Feet',\n",
      "         'AmazonBasics Bluetooth Keyboard for Android Devices - Black']\n",
      "Length: 9, dtype: str\n"
     ]
    }
   ],
   "source": [
    "# Assign Meta Categories\n",
    "\n",
    "def assign_meta_category(name):\n",
    "    \"\"\"\n",
    "    Assign products to one of 5 meta-categories based on name keywords.\n",
    "    Uses the category hierarchy determined during EDA.\n",
    "    \"\"\"\n",
    "    if pd.isna(name):\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    name_lower = name.lower()\n",
    "    \n",
    "    # Order matters — Kids must come before general Tablets\n",
    "    if any(kw in name_lower for kw in [\"kids edition\", \"kid-proof\"]):\n",
    "        return \"Fire Kids Edition\"\n",
    "    \n",
    "    elif any(kw in name_lower for kw in [\"fire tablet\", \"fire hd\", \"fire 7\", \n",
    "                                          \"fire 8\", \"fire 10\", \" fire \", \"tablet\"]):\n",
    "        return \"Fire Tablets\"\n",
    "    \n",
    "    elif any(kw in name_lower for kw in [\"kindle\", \"e-reader\", \"ebook\", \n",
    "                                          \"paperwhite\", \"voyage\", \"oasis\"]):\n",
    "        return \"Kindle E-Readers\"\n",
    "    \n",
    "    elif any(kw in name_lower for kw in [\"echo\", \"tap\", \"alexa\"]):\n",
    "        return \"Echo & Smart Speakers\"\n",
    "    \n",
    "    elif any(kw in name_lower for kw in [\"fire tv\", \"firetv\", \"streaming\", \n",
    "                                          \"media player\"]):\n",
    "        return \"Fire TV & Streaming\"\n",
    "    \n",
    "    else:\n",
    "        return \"Accessories & Other\"\n",
    "\n",
    "# Apply to full dataset\n",
    "df[\"meta_category\"] = df[\"name\"].apply(assign_meta_category)\n",
    "\n",
    "print(\"Meta-category distribution:\")\n",
    "print(df[\"meta_category\"].value_counts())\n",
    "\n",
    "# Check what landed in \"Other\"\n",
    "other_products = df[df[\"meta_category\"] == \"Accessories & Other\"][\"name\"].unique()\n",
    "print(f\"\\nProducts in 'Accessories & Other': {len(other_products)}\")\n",
    "if len(other_products) < 20:\n",
    "    print(other_products)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86ce05da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meta_category</th>\n",
       "      <th>name</th>\n",
       "      <th>review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accessories &amp; Other</td>\n",
       "      <td>AmazonBasics 11.6-Inch Laptop Sleeve</td>\n",
       "      <td>BETTER THAN NOTHING, But not as good as the CA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accessories &amp; Other</td>\n",
       "      <td>AmazonBasics 16-Gauge Speaker Wire - 100 Feet</td>\n",
       "      <td>As advised. Came really fast Great feel and se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Accessories &amp; Other</td>\n",
       "      <td>AmazonBasics Backpack for Laptops up to 17-inches</td>\n",
       "      <td>This is a very basic, functional backpack that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Accessories &amp; Other</td>\n",
       "      <td>AmazonBasics Bluetooth Keyboard for Android De...</td>\n",
       "      <td>Like a lot of reviewers here, I struggled to f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accessories &amp; Other</td>\n",
       "      <td>AmazonBasics External Hard Drive Case</td>\n",
       "      <td>I have the Western Digital My Passport Ultra 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Kindle E-Readers</td>\n",
       "      <td>Kindle Paperwhite</td>\n",
       "      <td>This is my 2 nd kindle. I bought this because ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Kindle E-Readers</td>\n",
       "      <td>Kindle Paperwhite E-reader - White, 6 High-Res...</td>\n",
       "      <td>I purchased this for my son overseas as he had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Kindle E-Readers</td>\n",
       "      <td>Kindle PowerFast International Charging Kit (f...</td>\n",
       "      <td>I travel internationally at least once a year ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Kindle E-Readers</td>\n",
       "      <td>Kindle Voyage E-reader, 6 High-Resolution Disp...</td>\n",
       "      <td>Much better than my original Kindle. Lighter a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Kindle E-Readers</td>\n",
       "      <td>Oem Amazon Kindle Power Usb Adapter Wall Trave...</td>\n",
       "      <td>Is Amazon kidding me They want me to pay 19.99...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          meta_category                                               name  \\\n",
       "0   Accessories & Other               AmazonBasics 11.6-Inch Laptop Sleeve   \n",
       "1   Accessories & Other      AmazonBasics 16-Gauge Speaker Wire - 100 Feet   \n",
       "2   Accessories & Other  AmazonBasics Backpack for Laptops up to 17-inches   \n",
       "3   Accessories & Other  AmazonBasics Bluetooth Keyboard for Android De...   \n",
       "4   Accessories & Other              AmazonBasics External Hard Drive Case   \n",
       "..                  ...                                                ...   \n",
       "75     Kindle E-Readers                                  Kindle Paperwhite   \n",
       "76     Kindle E-Readers  Kindle Paperwhite E-reader - White, 6 High-Res...   \n",
       "77     Kindle E-Readers  Kindle PowerFast International Charging Kit (f...   \n",
       "78     Kindle E-Readers  Kindle Voyage E-reader, 6 High-Resolution Disp...   \n",
       "79     Kindle E-Readers  Oem Amazon Kindle Power Usb Adapter Wall Trave...   \n",
       "\n",
       "                                          review_text  \n",
       "0   BETTER THAN NOTHING, But not as good as the CA...  \n",
       "1   As advised. Came really fast Great feel and se...  \n",
       "2   This is a very basic, functional backpack that...  \n",
       "3   Like a lot of reviewers here, I struggled to f...  \n",
       "4   I have the Western Digital My Passport Ultra 2...  \n",
       "..                                                ...  \n",
       "75  This is my 2 nd kindle. I bought this because ...  \n",
       "76  I purchased this for my son overseas as he had...  \n",
       "77  I travel internationally at least once a year ...  \n",
       "78  Much better than my original Kindle. Lighter a...  \n",
       "79  Is Amazon kidding me They want me to pay 19.99...  \n",
       "\n",
       "[80 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Agregate reviews by product and category\n",
    "\n",
    "product_df = (\n",
    "    df.groupby([\"meta_category\", \"name\"])\n",
    "      .agg({\n",
    "          \"review_text\": lambda x: \" \".join(x.astype(str).head(20))\n",
    "      })\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "product_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0d8f95",
   "metadata": {},
   "source": [
    "## Prompt Engineering Strategy\n",
    "\n",
    "Goals:\n",
    "- Minimize hallucination\n",
    "- Avoid competitor mentions\n",
    "- Prevent generic marketing language\n",
    "- Force grounded, review-based summarization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9501fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(category, review_text):\n",
    "    return f\"\"\"\n",
    "<|system|>\n",
    "You are a critical but fair technology journalist.\n",
    "Only use information explicitly stated in the provided customer reviews.\n",
    "Do NOT introduce technical specifications or product details that are not mentioned.\n",
    "Do NOT use bullet points.\n",
    "Write in continuous paragraphs only.\n",
    "Avoid marketing language and generic praise.\n",
    "\n",
    "<|user|>\n",
    "Using only the information in the reviews below, write a 350–500 word blog-style article about Amazon Echo products.\n",
    "\n",
    "Your article must:\n",
    "1. Identify the main Echo product types mentioned and describe how customers experience them.\n",
    "2. Highlight concrete strengths with real-world examples from the reviews.\n",
    "3. Explain recurring complaints or limitations.\n",
    "4. End with a clear, opinionated recommendation that explains:\n",
    "   - Who should buy an Echo\n",
    "   - Who may find it frustrating\n",
    "\n",
    "Be analytical and grounded in customer language.\n",
    "Do not invent specifications.\n",
    "Do not use bullet points.\n",
    "\n",
    "Category: {category}\n",
    "\n",
    "Customer Reviews:\n",
    "{review_text[:2500]}\n",
    "\n",
    "<|assistant|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d66c191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_category_text(category, top_n_products=3):\n",
    "    products = (\n",
    "        product_df[product_df[\"meta_category\"] == category]\n",
    "        .head(top_n_products)\n",
    "    )\n",
    "    return \" \".join(products[\"review_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b525eebe",
   "metadata": {},
   "source": [
    "## Generation Pipeline\n",
    "\n",
    "- First sampling with one category to fine-tune prompt afterwards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5113dd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation Cell - Sampling with Single Category\n",
    "\n",
    "category = \"Echo & Smart Speakers\"\n",
    "\n",
    "echo_text = build_category_text(category, top_n_products=3)\n",
    "\n",
    "prompt = build_prompt(category, echo_text)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=450,\n",
    "    temperature=0.15,\n",
    "    top_p=0.9,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Remove prompt part\n",
    "generated_text = generated_text[len(prompt):]\n",
    "\n",
    "print(generated_text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f678ad",
   "metadata": {},
   "source": [
    "We generate two variants per category:\n",
    "- Controlled (low temperature)\n",
    "- Creative (higher temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "08c3a28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_article(category, temperature=0.15):\n",
    "    category_text = build_category_text(category, top_n_products=3)\n",
    "    prompt = build_prompt(category, category_text)\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=500,\n",
    "        temperature=temperature,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Remove prompt\n",
    "    generated_text = generated_text[len(prompt):].strip()\n",
    "    \n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2902d6",
   "metadata": {},
   "source": [
    "## Generating Articles for All Categories\n",
    "\n",
    "Temperature = 0.15 → more factual, grounded  \n",
    "Temperature = 0.35 → more narrative, expressive  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8b335b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating for: Fire Tablets\n",
      "Generating for: Kindle E-Readers\n",
      "Generating for: Fire Kids Edition\n",
      "Generating for: Echo & Smart Speakers\n",
      "Generating for: Accessories & Other\n",
      "Generating for: Fire TV & Streaming\n"
     ]
    }
   ],
   "source": [
    "all_categories = df[\"meta_category\"].unique()\n",
    "\n",
    "results = {}\n",
    "\n",
    "for category in all_categories:\n",
    "    print(f\"Generating for: {category}\")\n",
    "    \n",
    "    results[category] = {\n",
    "        \"controlled\": generate_article(category, temperature=0.15),\n",
    "        \"creative\": generate_article(category, temperature=0.35)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81689f4c",
   "metadata": {},
   "source": [
    "## Persisting Results\n",
    "\n",
    "Generated articles are saved to:\n",
    "outputs/models/generated_blogposts.txt\n",
    "\n",
    "This ensures reproducibility and version control.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d96e5b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../outputs/models/generated_blogposts.txt\n"
     ]
    }
   ],
   "source": [
    "# Create directory to save plots\n",
    "os.makedirs(\"../outputs/models/\", exist_ok=True)\n",
    "\n",
    "output_path = \"../outputs/models/generated_blogposts.txt\"\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for category, versions in results.items():\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "        f.write(f\"CATEGORY: {category}\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"---- CONTROLLED VERSION ----\\n\\n\")\n",
    "        f.write(versions[\"controlled\"] + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"---- CREATIVE VERSION ----\\n\\n\")\n",
    "        f.write(versions[\"creative\"] + \"\\n\\n\\n\")\n",
    "\n",
    "print(f\"Saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ef5b14",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3845ec6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_extractive_baseline(category, top_n_reviews=5, max_words=400):\n",
    "    \"\"\"\n",
    "    Simple extractive baseline:\n",
    "    - Select top N longest reviews in the category\n",
    "    - Concatenate and truncate to max_words\n",
    "    \"\"\"\n",
    "    reviews = df[df[\"meta_category\"] == category][\"review_text\"].dropna()\n",
    "    \n",
    "    # Sort by length (descending)\n",
    "    reviews = sorted(reviews, key=lambda x: len(x), reverse=True)\n",
    "    \n",
    "    selected = \" \".join(reviews[:top_n_reviews])\n",
    "    \n",
    "    words = selected.split()\n",
    "    \n",
    "    return \" \".join(words[:max_words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d741f21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "def compute_rouge(reference, generated):\n",
    "    scores = scorer.score(reference, generated)\n",
    "    \n",
    "    return {\n",
    "        \"rouge1\": scores[\"rouge1\"].fmeasure,\n",
    "        \"rouge2\": scores[\"rouge2\"].fmeasure,\n",
    "        \"rougeL\": scores[\"rougeL\"].fmeasure\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccaf46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bertscore(reference, generated):\n",
    "    P, R, F1 = bertscore(\n",
    "        [generated],\n",
    "        [reference],\n",
    "        lang=\"en\",\n",
    "        rescale_with_baseline=False   # ← important\n",
    "    )\n",
    "    \n",
    "    return float(F1.mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "750c7b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_compression_ratio(source_text, generated_text):\n",
    "    source_len = len(source_text.split())\n",
    "    generated_len = len(generated_text.split())\n",
    "    \n",
    "    return generated_len / source_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d0ebea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
    "    return set(text.split())\n",
    "\n",
    "def compute_grounding_ratio(source_text, generated_text):\n",
    "    source_tokens = tokenize(source_text)\n",
    "    generated_tokens = tokenize(generated_text)\n",
    "    \n",
    "    overlap = generated_tokens.intersection(source_tokens)\n",
    "    \n",
    "    if len(generated_tokens) == 0:\n",
    "        return 0\n",
    "    \n",
    "    return len(overlap) / len(generated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c848487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work with the generated_blogsposts.txt to avoid re-train the model for evaluation purposes only\n",
    "\n",
    "filepath = \"../outputs/models/generated_blogposts.txt\"\n",
    "\n",
    "results = parse_generated_file(filepath)\n",
    "\n",
    "print(results.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec61f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_generated_file(filepath):\n",
    "    results = {}\n",
    "    \n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Split by CATEGORY blocks\n",
    "    category_blocks = re.split(r\"={5,}\\nCATEGORY:\\s*\", content)\n",
    "    \n",
    "    for block in category_blocks:\n",
    "        if not block.strip():\n",
    "            continue\n",
    "        \n",
    "        # First line is category name\n",
    "        lines = block.strip().split(\"\\n\")\n",
    "        category = lines[0].strip()\n",
    "        \n",
    "        # Extract controlled and creative versions\n",
    "        controlled_match = re.search(\n",
    "            r\"---- CONTROLLED VERSION ----\\n\\n(.*?)\\n\\n---- CREATIVE VERSION ----\",\n",
    "            block,\n",
    "            re.DOTALL\n",
    "        )\n",
    "        \n",
    "        creative_match = re.search(\n",
    "            r\"---- CREATIVE VERSION ----\\n\\n(.*)\",\n",
    "            block,\n",
    "            re.DOTALL\n",
    "        )\n",
    "        \n",
    "        controlled_text = controlled_match.group(1).strip() if controlled_match else \"\"\n",
    "        creative_text = creative_match.group(1).strip() if creative_match else \"\"\n",
    "        \n",
    "        results[category] = {\n",
    "            \"controlled\": controlled_text,\n",
    "            \"creative\": creative_text\n",
    "        }\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d5dd877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: Fire Tablets\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cebd1ed3b87248af8dc5d4151092b4ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/389 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRobertaModel LOAD REPORT\u001b[0m from: roberta-large\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "pooler.dense.bias               | MISSING    | \n",
      "pooler.dense.weight             | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: Kindle E-Readers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "930f1a01e10b41f0b7aac5212b509a43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/389 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRobertaModel LOAD REPORT\u001b[0m from: roberta-large\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "pooler.dense.bias               | MISSING    | \n",
      "pooler.dense.weight             | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: Fire Kids Edition\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4b48ac8eeb3410997ea778578457f68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/389 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRobertaModel LOAD REPORT\u001b[0m from: roberta-large\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "pooler.dense.bias               | MISSING    | \n",
      "pooler.dense.weight             | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: Echo & Smart Speakers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e2ebee4c0ae4674a93ff5dcdbb8330a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/389 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRobertaModel LOAD REPORT\u001b[0m from: roberta-large\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "pooler.dense.bias               | MISSING    | \n",
      "pooler.dense.weight             | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: Accessories & Other\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75e0c5f5678e4ea7bbe013967b2e720d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/389 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRobertaModel LOAD REPORT\u001b[0m from: roberta-large\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "pooler.dense.bias               | MISSING    | \n",
      "pooler.dense.weight             | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: Fire TV & Streaming\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9688f6296e0f43d4a293786761c5fb19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/389 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRobertaModel LOAD REPORT\u001b[0m from: roberta-large\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "pooler.dense.bias               | MISSING    | \n",
      "pooler.dense.weight             | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "evaluation_results = []\n",
    "\n",
    "for category in results.keys():\n",
    "    \n",
    "    print(f\"Evaluating: {category}\")\n",
    "    \n",
    "    # Extractive pseudo-reference\n",
    "    reference = build_extractive_baseline(category)\n",
    "    \n",
    "    # Generated text\n",
    "    generated = results[category][\"controlled\"]\n",
    "    \n",
    "    # ROUGE\n",
    "    rouge_scores = compute_rouge(reference, generated)\n",
    "    \n",
    "    # BERTScore\n",
    "    bert_f1 = compute_bertscore(reference, generated)\n",
    "    \n",
    "    # Compression ratio\n",
    "    source_text = build_category_text(category, top_n_products=3)\n",
    "    compression = compute_compression_ratio(source_text, generated)\n",
    "\n",
    "    # Grounding Ratio\n",
    "    grounding = compute_grounding_ratio(source_text, generated)\n",
    "    \n",
    "    evaluation_results.append({\n",
    "        \"category\": category,\n",
    "        \"rouge1\": rouge_scores[\"rouge1\"],\n",
    "        \"rouge2\": rouge_scores[\"rouge2\"],\n",
    "        \"rougeL\": rouge_scores[\"rougeL\"],\n",
    "        \"bertscore_f1\": bert_f1,\n",
    "        \"compression_ratio\": compression,\n",
    "        \"grounding_ratio\": grounding\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e68b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>bertscore_f1</th>\n",
       "      <th>compression_ratio</th>\n",
       "      <th>grounding_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fire Tablets</td>\n",
       "      <td>0.292135</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.126404</td>\n",
       "      <td>0.800121</td>\n",
       "      <td>0.176573</td>\n",
       "      <td>0.509934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kindle E-Readers</td>\n",
       "      <td>0.304993</td>\n",
       "      <td>0.024357</td>\n",
       "      <td>0.137652</td>\n",
       "      <td>0.808534</td>\n",
       "      <td>0.227821</td>\n",
       "      <td>0.463087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fire Kids Edition</td>\n",
       "      <td>0.332948</td>\n",
       "      <td>0.034762</td>\n",
       "      <td>0.145665</td>\n",
       "      <td>0.805684</td>\n",
       "      <td>0.177215</td>\n",
       "      <td>0.543624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Echo &amp; Smart Speakers</td>\n",
       "      <td>0.343008</td>\n",
       "      <td>0.031746</td>\n",
       "      <td>0.137203</td>\n",
       "      <td>0.825337</td>\n",
       "      <td>0.180735</td>\n",
       "      <td>0.630303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accessories &amp; Other</td>\n",
       "      <td>0.357895</td>\n",
       "      <td>0.065651</td>\n",
       "      <td>0.152047</td>\n",
       "      <td>0.797558</td>\n",
       "      <td>0.194293</td>\n",
       "      <td>0.578947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fire TV &amp; Streaming</td>\n",
       "      <td>0.381963</td>\n",
       "      <td>0.106383</td>\n",
       "      <td>0.156499</td>\n",
       "      <td>0.830480</td>\n",
       "      <td>1.283582</td>\n",
       "      <td>0.464481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                category    rouge1    rouge2    rougeL  bertscore_f1  \\\n",
       "0           Fire Tablets  0.292135  0.014085  0.126404      0.800121   \n",
       "1       Kindle E-Readers  0.304993  0.024357  0.137652      0.808534   \n",
       "2      Fire Kids Edition  0.332948  0.034762  0.145665      0.805684   \n",
       "3  Echo & Smart Speakers  0.343008  0.031746  0.137203      0.825337   \n",
       "4    Accessories & Other  0.357895  0.065651  0.152047      0.797558   \n",
       "5    Fire TV & Streaming  0.381963  0.106383  0.156499      0.830480   \n",
       "\n",
       "   compression_ratio  grounding_ratio  \n",
       "0           0.176573         0.509934  \n",
       "1           0.227821         0.463087  \n",
       "2           0.177215         0.543624  \n",
       "3           0.180735         0.630303  \n",
       "4           0.194293         0.578947  \n",
       "5           1.283582         0.464481  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df = pd.DataFrame(evaluation_results)\n",
    "\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17576bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rouge1               0.335490\n",
       "rouge2               0.046164\n",
       "rougeL               0.142578\n",
       "bertscore_f1         0.811286\n",
       "compression_ratio    0.373370\n",
       "grounding_ratio      0.531729\n",
       "dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.mean(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7859fb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
    "    return set(text.split())\n",
    "\n",
    "def compute_grounding_ratio(source_text, generated_text):\n",
    "    source_tokens = tokenize(source_text)\n",
    "    generated_tokens = tokenize(generated_text)\n",
    "    \n",
    "    overlap = generated_tokens.intersection(source_tokens)\n",
    "    \n",
    "    if len(generated_tokens) == 0:\n",
    "        return 0\n",
    "    \n",
    "    return len(overlap) / len(generated_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200a106f",
   "metadata": {},
   "source": [
    "## Evaluation & Observations\n",
    "\n",
    "### Model Behavior & Prompting\n",
    "\n",
    "Compared to TinyLlama, LLaMA 3.2-3B produced more coherent, structurally consistent, and grounded summaries. Early prompt iterations introduced competitor brands and generic phrasing; tightening instructions and lowering temperature significantly reduced this behavior.\n",
    "\n",
    "Controlled decoding (low temperature) yielded more stable and grounded outputs, while higher temperature decoding improved stylistic richness at the cost of slight drift risk.\n",
    "\n",
    "### Quantitative Evaluation\n",
    "\n",
    "Because no human-written reference summaries were available, extractive baselines were constructed from the longest reviews per category. Generated summaries were evaluated using ROUGE and BERTScore.\n",
    "\n",
    "ROUGE-1 ≈ 0.33 reflects moderate lexical overlap, consistent with abstractive summarization where wording differs from source text.  \n",
    "BERTScore F1 ≈ 0.81 indicates strong semantic alignment between generated summaries and underlying reviews.\n",
    "\n",
    "The average compression ratio of 0.37 demonstrates effective condensation of multi-review inputs into concise recommendation-style articles. Smaller categories exhibited expansion rather than compression due to limited source material.\n",
    "\n",
    "### Grounding & Hallucination Proxy\n",
    "\n",
    "Lexical grounding (overlap between generated and source vocabulary) averaged ≈0.53. Over half of generated vocabulary was directly anchored in review text, with remaining differences largely attributable to narrative framing and recommendation language. While this metric does not capture semantic hallucination, it provides a coarse but useful signal of source alignment.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "Generation quality remains sensitive to dataset noise and cross-category review contamination. Automated metrics provide directional insight but do not fully capture coherence, recommendation strength, or factual faithfulness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af91a933",
   "metadata": {},
   "source": [
    "## Final Conclusions\n",
    "\n",
    "This notebook demonstrates that LLM-based category summarization is feasible using constrained prompting and controlled decoding. Compared to TinyLlama, LLaMA produced more coherent, structurally consistent, and better-grounded summaries.\n",
    "\n",
    "Dataset noise (including cross-product review contamination) required product-level aggregation to stabilize generation. Prompt constraints and low-temperature decoding further reduced drift and competitor hallucinations.\n",
    "\n",
    "Quantitative evaluation showed moderate lexical overlap (ROUGE-1 ≈ 0.33) and strong semantic alignment (BERTScore ≈ 0.81), with effective compression of multi-review inputs. Grounding analysis (≈0.53 lexical overlap) suggests outputs remained largely anchored to source content despite narrative rephrasing.\n",
    "\n",
    "Overall, LLM-based summarization generated meaningful, interpretable category insights with manageable hallucination risk under structured prompting.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenAI Base (py311)",
   "language": "python",
   "name": "openai-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
